{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc05c3ca-ae18-4ce7-91fd-2f99401c9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ed19cd-c9be-4eab-a7fa-980c7644e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205d2e92-bdcd-4650-a67e-293c2f45b995",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# we need to keep require_grad param True else we will get error like this\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Data Science\\pytorch_basics\\pytorch-basics\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Data Science\\pytorch_basics\\pytorch-basics\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Data Science\\pytorch_basics\\pytorch-basics\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# we need to keep require_grad param True else we will get error like this\n",
    "a.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f10fb9b-a38e-40b6-be30-de753274808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using autograd to take derivative\n",
    "a = torch.tensor(3.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e305bf1b-8f48-448c-9790-9bcf320e902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d573deac-949b-4a41-a9ee-fd4b31fe1e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a717eb5-6d19-4eef-885c-7c525786a79c",
   "metadata": {},
   "source": [
    "### Taking derivative for two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c9445de-c9dc-4dc8-826f-86d479876e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0789a01-a00f-411f-bd1e-0b05f5c3aae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5., requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eea88b14-a654-44d6-a50c-a68ce40ce7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17ce26a1-5d85-4804-b886-a2af8eb80126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7786a57-045a-431b-b004-46099f6d2626",
   "metadata": {},
   "source": [
    "- when we are take gradient of y according to x, it becomes 2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "178bcab7-7eb0-4a99-983e-ed736e1bc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1c01336-471b-4f3b-af02-b4cefd899a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49e405-9b57-4d1c-b426-5f4f100a3e55",
   "metadata": {},
   "source": [
    "### Taking Partial derivatives with three variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7d63be-69d1-429c-a4f9-6d59eb1f908c",
   "metadata": {},
   "source": [
    "- Let we are finding the partial derivative of z with respect to x\n",
    "    - we need to take derivative of z with respect to y and then y with respect to x\n",
    "        - x = 4, y = x^2, z = sin(y) and we need to get dz/dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa093b1c-7f86-40cc-8899-d752eb9c1e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c890d9a7-7707-4aaf-b111-c00dfce4e5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86fd6731-b130-462e-a8bc-9b0236b1735e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.7568, grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.sin(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a9216-e768-41c2-bddb-002d50bd0aa6",
   "metadata": {},
   "source": [
    "- we can see that how each variable is keeping track for derivative according to its previous var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36e4dfe5-0c69-48db-9112-7ed41413ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a20fbcb7-30a9-4b25-881e-e4a4560dae13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6146)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33833ec-9c0b-4f74-b677-059cc5cb3263",
   "metadata": {},
   "source": [
    "### Performing derivative process for a perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "292f67e9-0305-4f88-bcf0-5cea630badd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.5) # input\n",
    "y = torch.tensor(0.0) # output\n",
    "\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True) # weight\n",
    "b = torch.tensor(0.0, requires_grad=True) # bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cb6cc53-4e40-4b0c-87af-7c31310464af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    epsilon = 1e-8  # To prevent log(0)\n",
    "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2b9f572-cf9b-4d5f-83fa-b09559e36f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b9d881a-f334-4613-8a0c-eb79ee96d654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = w*x + b\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d93314a1-6359-4dd3-a147-2e749fc8369b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9707, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = torch.sigmoid(z)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35fc0933-5521-494f-8626-6e9bf8345e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = binary_cross_entropy_loss(y_pred,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9301b5dd-f6f8-44ea-aa98-f522b45627a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5297, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e0fa640-ca54-4b48-b0d4-299b8864261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b17cbeeb-e96a-4b2c-a8fd-7e03c8132999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3974)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8c1d8b9-d2af-416f-9b51-ea66b46517e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9707)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f21450-9351-4289-ae4a-ef0b00fe44f0",
   "metadata": {},
   "source": [
    "### Performing on tensors not scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6e11657-7b8a-4c7e-b0af-065c7f1e86f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 5.], requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2.0,3.0,5.0], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d421a4de-d346-4c08-a447-43d93108b6da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (x**2).mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72c8be9e-dd09-4737-8fc1-6eb4ffcd55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9c12e7dc-efb1-4375-b9ea-51e64416c5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3333, 2.0000, 3.3333])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e2388-2524-446f-ab19-b62692b131f5",
   "metadata": {},
   "source": [
    "### Clearing grad\n",
    "- It is important to clear gradient else they keep adding to the previous ones and we don't get the current gradient else it's the additon of previous ones as many time we call backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6432926-e67f-4c56-b692-4a57dcefb391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "18e0a794-3eaf-454e-bc28-d36cb33adf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a0c1c66e-f0aa-45ca-89f3-14f79ced1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e395603c-0491-4c97-a09e-ad16de4f0fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "59145c1d-5014-405c-904e-039179f32f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to clear the gradients\n",
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da90f01-c061-406f-8fd2-4343aaa26c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2230d-193f-4972-b3f0-230ff3304ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
